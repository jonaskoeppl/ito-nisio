Unter der zusätzlichen Voraussetzung, dass die unabhängigen Zufallsvariablen $(X_n)_{n \in \N}$ symmetrisch verteilt sind, lässt sich der Satz von Itô-Nisio auf drei noch schwächere Annahmen als die Verteilungskonvergenz von $(S_n)_{n \in \N}$ erweitern. 
Für den Beweis dient uns unter anderem der \mbox{folgende Satz.} 
\begin{theorem}
    Sei $(X_n)_{n \in \N}$ eine Folge unabhängiger Radon-Zufallsvariablen und sei $(\mu_n)_{n \in \N}$ gleichmäßig straff. Dann existiert eine Folge $(c_n)_{n \in \N}$ in $E$, sodass $(S_n - c_n)_{n \in \N}$ fast sicher konvergiert.
\end{theorem}

\begin{proof*}
    Betrachte den Produktraum $(\Omega \times \Omega, \mathcal{A} \otimes \mathcal{A}, P \times P)$ und definiere für $n \in \N$
    \begin{align*}
        \widetilde{X}_n&: \Omega \times \Omega \to E, \ (\omega_1, \omega_2) \mapsto X_n(\omega_1), \\\
        \widetilde{Y}_n&: \Omega \times \Omega \to E, \ (\omega_1, \omega_2) \mapsto X_n(\omega_2),
    \end{align*}
    sowie 
    \begin{align*}
        \widetilde{S}_n := \sum_{i = 1}^n \widetilde{X}_i, \quad \widetilde{T}_n := \sum_{i = 1}^n \widetilde{Y}_n, \quad U_n := \widetilde{S}_n - \widetilde{T}_n, \quad \mu_{U_n} := (P\times P)^{U_n}. 
    \end{align*}
    Nach Konstruktion sind die Zufallsvariablen $\widetilde{S}_n$, $\widetilde{T}_n$ und $S_n$ für festes $n \in \N$ identisch verteilt. Wir zeigen zunächst, dass $\{\mu_{U_n}: \ n \in \N\}$ gleichmäßig straff ist. 
    Sei dazu $\varepsilon > 0$. Dann existiert nach Voraussetzung eine kompakte Menge $\tilde{K} \subseteq E$ mit 
    $$
        \forall n \in \N: \mu_n(\tilde{K}) \geq 1 - \varepsilon. 
    $$
    Wegen der Stetigkeit der Abbildung 
    $$
        E \times E \to E, \ (x,y) \mapsto x - y
    $$
    ist auch die Menge $K := \{ x - y : x,y \in \tilde{K}\}$ kompakt und somit insbesondere messbar. Ferner gilt
    \begin{align*}
        \mu_{U_n}(K) = (P \times P)\{\widetilde{S}_n - \widetilde{T}_n \in K\} &\geq (P \times P)\{\widetilde{S}_n \in \tilde{K}, \widetilde{T}_n \in \tilde{K}\} \\\
                                                              &\geq 1 - (P \times P)\{\widetilde{S}_n \notin \tilde{K}\} - (P \times P)\{\widetilde{T}_n \notin \tilde{K}\} \\\
                                                              &= 1 - 2\mu_n(\tilde{K}^c) \\\
                                                              &\geq 1 - 2 \varepsilon.                                              
    \end{align*}
    Demnach ist $\{\mu_{U_n}: \ n \in \N\}$ straff. 
    Als nächstes zeigen wir, dass $(\widehat{\mu_{U_n}}(f))_{n \in \N}$ für alle $f \in E'$ konvergiert. Sei dazu $f \in E'$ beliebig aber fest. 
    Die Unabhängigkeit von $(\widetilde{Y}_n)_{n \in \N}$ und $(\widetilde{X}_n)_{n \in \N}$ liefert direkt die Unabhängigkeit von $(\widetilde{X}_n - \widetilde{Y}_n)_{n \in \N}$
    und nach Konstruktion sind $\tilde{Y}_n$ und $\tilde{X}_n$ für $n \in \N$ identisch verteilt. Es gilt demzufolge  
    \begin{align*}
        \widehat{\mu_{U_n}}(f) = E(e^{if(U_n)}) &= \mathbb{E}\big(\prod_{j=1}^n(e^{if(\widetilde{X}_j-\widetilde{Y}_j)})\big) \\\
                                                &= \prod_{j=1}^n \mathbb{E}(e^{if(\widetilde{X}_j-\widetilde{Y}_j)})
                                                 = \prod_{j=1}^n \mathbb{E}(e^{if(\widetilde{X}_j)})\mathbb{E}(e^{-if(\widetilde{Y}_j)})
                                                 = \prod_{j=1}^n \abs{\mathbb{E}(e^{if(\widetilde{X}_j)})}^2.
    \end{align*}
    Wegen $0 \leq \abs{\mathbb{E}(e^{if(\widetilde{X}_j)})} \leq 1$ für alle $j \in \N$ folgt daraus die Konvergenz von $(\widehat{\mu_{U_n}}(f))_{n \in \N}$. 
    Wir zeigen nun, dass die Folge $(\mu_{U_n})_{n \in \N}$ schwach konvergiert. 
    Wie zuvor gezeigt wurde, ist $\{\mu_{U_n}: n \in \N\}$ gleichmäßig straff, also nach Satz $1.27$ relativ kompakt in $(\mathcal{M}(E), \rho)$. 
    Es genügt also zu zeigen, dass alle konvergenten Teilfolgen von $(\mu_{U_n})_{n \in \N}$ gegen den gleichen Grenzwert konvergieren. 
    Seien dazu $(\nu_n)_{n \in \N}$ und $(\eta_n)_{n \in \N}$ zwei konvergente Teilfolgen von $(\mu_{U_n})_{n \in \N}$ mit schwachem Grenzwert $\nu$ bzw. $\eta$. 
    Insbesondere gilt also nach Satz $2.17$ für alle $f \in E'$
    $$
        \lim_{n \to \infty} \widehat{\nu_n}(f) = \widehat{\nu}(f) \text{ und } \lim_{n \to \infty} \widehat{\eta_n}(f) = \widehat{\eta}(f). 
    $$
    Da für alle $f \in E'$ die Folge $(\widehat{\mu_{U_n}}(f))_{n \in \N}$ konvergiert, gilt 
    $$
        \forall f \in E': \ \nu(f) = \lim_{n \to \infty} \widehat{\nu_n}(f) = \lim_{n \to \infty}\widehat{\mu_{U_n}}(f) = \lim_{n \to \infty}\widehat{\eta_n}(f) = \widehat{\eta}(f). 
    $$
    Nach Satz $2.16$ folgt daraus $\eta = \nu$. Also ist die Folge $(\mu_{U_n})_{n \in \N}$ schwach konvergent, d.h. $(U_n)_{n \in \N}$ konvergiert in Verteilung. 
    Aus Satz $3.6$ folgt nun, dass $(U_n)_{n \in \N}$ fast sicher konvergiert. 
    Daher existiert eine Menge $\Omega^* \in \mathcal{A} \otimes \mathcal{A}$ mit $(P\times P)(\Omega^*) = 1$ und 
    $$
        \forall (\omega_1, \omega_2) \in \Omega^*: \quad (U_n(\omega_1, \omega_2))_{n \in \N} = (S_n(\omega_1) - S_n(\omega_2))_{n \in \N} \text{ konvergiert.}
    $$
    Mit dem Satz von Fubini erhalten wir wie im Beweis von Satz $3.6 \ (ii) \Rightarrow (i)$ ein $\omega' \in \Omega$, sodass \mbox{$(S_n - S_n(\omega'))_{n \in \N}$} fast sicher konvergiert. 
    Die Folge $(c_n)_{n \in \N}$, definiert durch $c_n := S_n(\omega')$, $n \in \N$, erfüllt nun die gewünschte Eigenschaft. \qed

\end{proof*}

\begin{theorem}[Satz von Itô-Nisio für Folgen symmetrischer Zufallsvariablen]
    Sei $(X_n)_{n \in \N}$ eine Folge unabhängiger und symmetrischer Zufallsvariablen in $\mathcal{L}_0(E)$. Dann sind äquivalent:
    \begin{enumerate}[(i)]
        \item $(S_n)_{n \in \N}$ konvergiert fast sicher.
        \item $(S_n)_{n \in \N}$ konvergiert stochastisch. 
        \item $(S_n)_{n \in \N}$ konvergiert in Verteilung. 
        \item $(\mu_n)_{n \in \N}$ ist gleichmäßig straff.
        \item Es gibt eine Zufallsvariable $S \in \mathcal{L}_0(E)$, sodass 
        $$
            \forall f \in E': \quad f(S_n) \stochastisch f(S).
        $$
        \item Es gibt ein Wahrscheinlichkeitsmaß $\mu$ auf $\mathcal{B}(E)$, sodass 
        $$
            \forall f \in E': \quad \lim_{n \to \infty}\widehat{\mu_n}(f) = \widehat{\mu}(f). 
        $$
    \end{enumerate}
\end{theorem}

\begin{proof*}
    Die Äquivalenz $(i) \iff (ii) \iff (iii)$ folgt aus Satz $3.6$. Die Implikation $(iii) \Rightarrow (iv)$ gilt nach Satz $1.27$. Ferner erhalten wir $(i) \Rightarrow (v)$ aus Proposition $2.12$.
    Wir zeigen noch $(v)\Rightarrow (vi)$, $(vi) \Rightarrow (iv)$ und $(v) \Rightarrow (iv) \Rightarrow (i)$. 
    \newline
    Zu $(v)\Rightarrow (vi)$: Setze $\mu := P^S$ und sei $f \in E'$. Wegen Satz $2.6$ können wir ohne Einschränkung annehmen, dass $(f(S_n))_{n \in \N}$ fast sicher gegen $f(S)$ konvergiert. 
    Der Satz von der dominierten Konvergenz liefert dann 
    $$
        \lim_{n \to \infty}\widehat{\mu_n}(f) = \lim_{n \to \infty}\mathbb{E}(e^{if(S_n)}) = \mathbb{E}(e^{f(S)}) = \widehat{\mu}(f).
    $$
    \newline
    Zu $(iv) \Rightarrow (i)$:
    Nach Satz $3.8$ existiert eine Folge $(c_n)_{n \in \N}$ in $E$, sodass $(S_n - c_n)_{n \in \N}$ fast sicher konvergiert. Setze nun $P^X := P^{(X_1,X_2,...)}$ und $P^{-X} :=P^{(-X_1,-X_2,...)}$. 
    Dann sind $P^X$ und $P^{-X}$ jeweils Wahrscheinlichkeitsmaße auf dem Produktraum $(E^{\N}, \otimes_{n \in \N}B(E))$ und wegen der Unabhängigkeit und Symmetrie von $(X_n)_{n \in \N}$ erhalten wir direkt $P^X = P^{-X}$. Daher gilt für $N \in \N$ und $\varepsilon >0$
    \begin{align*}
        &\quad \ \prob{\sup_{n \geq N}\norm{(S_n - c_n) - (S_N- c_N)} > \varepsilon} \\\
                &= P^X\big(\{(y_n)_{n \in \N}\in E^{\N}: \  \sup_{n \geq N}\norm{y_n + y_{n-1} + ... + y_{N+1} + (c_n - c_N)} > \varepsilon \}\big) \\\
                &= P^{-X}\big(\{(y_n)_{n \in \N}\in E^{\N}: \ \sup_{n \geq N}\norm{y_n + y_{n-1} + ... + y_{N+1} + (c_n - c_N)} > \varepsilon \}\big) \\\
                &= \prob{\sup_{n \geq N}\norm{(-S_n - c_n) - (-S_N - c_N)} > \varepsilon}.
    \end{align*}
    Nach Satz $2.4$ konvergiert also auch $(-S_n - c_n)_{n \in \N}$ fast sicher. Daraus folgt die fast sichere Konvergenz von $(S_n)_{n \in \N}$, denn für $n \in \N$ gilt
    $$
       S_n =  \frac{1}{2}((S_n - c_n) - (-S_n -c_n)). 
    $$
    Zu $(v) \Rightarrow (iv)$: Wegen der Unabhängigkeit von $(X_n)_{n \in \N}$ sind für alle $f \in E'$ und $m \geq n$ die Zufallsvariablen $f(S_m - S_n)$ und $f(S_n)$ unabhängig. 
    Nach Proposition $2.19$ sind somit $f(S-S_n)$ und $f(S_n)$ für alle $n \in \N$ unabhängig. Zusammen mit der Symmetrie von $S_n$ ergibt sich deshalb für $f \in E'$
    \begin{align*}
        \widehat{P^S}(f) = \mathbb{E}(e^{if(S)}) &= \mathbb{E}(e^{if(S-S_n)})\mathbb{E}(e^{if(S_n)})  \\\
                                        &= \mathbb{E}(e^{if(S-S_n)})\mathbb{E}(e^{if(-S_n)}) = \mathbb{E}(e^{if(S-2S_n)}) = \widehat{P^{S-2S_n}}(f).
    \end{align*}
    Folglich sind $S$ und $S - 2S_n$ nach Satz $2.16$ für alle $n \in \N$  identisch verteilt. Da $P^S$ straff ist, existiert zu $\varepsilon >0$ eine kompakte Menge $K \subseteq E'$ mit $\prob{S \in K} \geq 1 - \varepsilon$. 
    Aus Stetigkeitsgründen ist auch die Menge $L := \{\frac{1}{2}(x-y): x,y \in K\}$ kompakt und es gilt für alle $n \in \N$
    $$
        \prob{S_n \in L} \geq \prob{S \in K, \ S-2S_n \in K} \geq 1 - \prob{S \notin K} - \prob{S-2S_n \notin K} \geq 1 - 2\varepsilon. 
    $$ 
    Somit ist $(\mu_n)_{n \in \N}$ gleichmäßig straff. 
    \newline
    Zu $(vi) \Rightarrow (iv)$: 
    Sei $f \in E'$ beliebig aber fest. Betrachte die Abbildungen 
    \begin{align*}
        \varphi_n &: \R \to \C, \quad t \mapsto \int_E e^{itf(x)}\mu_n(dx) = \widehat{\mu_n}(tf), \quad n \in \N, \\\
        \varphi   &: \R \to \C, \quad t \mapsto \int_E e^{itf(x)}\mu(dx) = \widehat{\mu}(tf). 
    \end{align*}
    Dann ist $\varphi_n$ für alle $n \in \N$ die charakteristische Funktion von $\mu_n^{f}$ und $\varphi$ die charakteristische Funktion von $\mu^f$. Nach Voraussetzung konvergiert $(\varphi_n)_{n \in \N}$ punktweise gegen $\varphi$ und nach 
    dem Stetigkeitssatz von Lévy, vgl. \cite[Satz 8.7.5]{gs}, gilt daher $\mu_n^f \rightharpoonup \mu^f$. Für festes $f \in E'$ ist $\{\mu_n^f : n \in \N\}$ somit insbesondere relativ kompakt.
     Nach Satz $1.33$ genügt es folglich zu zeigen, dass $(\mu_n)_{n \in \N}$ flach konzentriert ist. 
    Da $\{\mu\}$ flach konzentriert ist, genügt es dafür zu zeigen, dass für jeden endlichdimensionalen Untervektorraum $F \subseteq E$ und alle $\varepsilon > 0$ gilt 
    $$
        \mu_n(\{x \in E: \ \inf_{y \in F}\norm{x-y} > \varepsilon \}) \leq 2 \mu(\{x \in E: \ \inf_{y \in F}\norm{x-y} > \varepsilon \}).
    $$
    Man beachte hierbei, dass die Menge $\{x \in E: \ \inf_{y \in F}\norm{x-y} > \varepsilon \}$ als offene Menge insbesondere messbar ist. 
    Sei also $F \subseteq E$ ein endlichdimensionaler Untervektorraum und $\varepsilon >0$. Nach Korollar $A.13$ existiert eine Folge $(f_n)_{n \in \N}$ in $E'$ mit 
    $$
        \forall x \in E: \quad d(x,F) := \inf_{y \in F}\norm{x-y} = \sup_{n \in \N}\abs{f_n(x)}. 
    $$
    Sei zunächst $m \in \N$ festgewählt. Mittels charakteristischer Funktionen prüft man leicht, dass $\mu_n^{(f_1,...,f_m)} \rightharpoonup \mu^{(f_1,...,f_m)}$. 
    Wegen der Linearität von $f_1,...,f_m$ können wir Satz $3.6$ auf die Folge $(T_n^{(m)})_{n \in \N} := ((f_1,...,f_m)\circ S_n)_{n \in \N}$ anwenden und erhalten eine $\R^m$-wertige Zufallsvariable $T^{(m)}$ auf $(\Omega, \mathcal{A}, P)$ mit 
    $T_n^{(m)} \stochastisch T^{(m)}$. Insbesondere gilt daher $T_n^{(m)} \schwach T^{(m)}$ und somit \mbox{$P^{T^{(m)}} = \mu^{(f_1,...,f_m)}$}. 
    Ferner ist $T_n$ wegen der Linearität von $f_1,...,f_m$ und der Symmetrie von $S_n$ für alle $n \in \N$ symmetrisch . Mit Korollar $3.4$, angewendet auf $(T_n^{(m)})_{n \in \N}$, erhalten wir im Banachraum $(\R^m, \norm{\cdot}_{\infty})$ für fast alle $\varepsilon >0$
    \begin{align*}
        \prob{\max_{1 \leq i \leq m}\abs{f_i(S_n)} > \varepsilon} &= \prob{\norm{T_n^{(m)}}_{\infty} > \varepsilon} \\\
                                                                  &\leq 2 \prob{\norm{T^{(m)}}_{\infty} > \varepsilon} = 2\mu\big(\{x \in E: \max_{1\leq i \leq m}\abs{f_i(x)} > \varepsilon\}\big).
    \end{align*}
    Mit der $\sigma$-Stetigkeit von Wahrscheinlichkeitsmaßen folgt schließlich
    \begin{align*}
        \mu_n(\{x \in E: \ \inf_{y \in F}\norm{x-y} > \varepsilon \}) = \prob{d(S_n, F) > \varepsilon}
                                                                     &= \lim_{m \to \infty}\prob{\max_{1 \leq i \leq m}\abs{f_i(S_n)} > \varepsilon} \\\
                                                                    &\leq \lim_{m \to \infty} 2 \mu(\{x \in E: \max_{1\leq i \leq m}\abs{f_i(x)} > \varepsilon\}) \\\
                                                                    &= 2 \mu(\{x \in E: \ \inf_{y \in F}\norm{x-y} > \varepsilon \}).
    \end{align*}
    \qed
\end{proof*}

\begin{remark}
    Auf die Annahme der Symmetrie in Satz $3.9$ kann im Allgemeinen nicht verzichtet werden. Sei etwa $(E, \langle\cdot,\cdot\rangle)$ ein Hilbertraum mit Orthonormalbasis $(e_n)_{n \in \N}$.
    Man bemerke zunächst, dass nach dem Darstellungsatz von Fréchet-Riesz, vgl. \cite[Theorem V.3.6]{werner}, für jedes $f \in E'$ ein $z \in E$ mit $f = \langle \cdot, z \rangle$ existiert. 
    Setze nun 
    $$
        X_1(\omega) = e_1, \quad X_n(\omega) = e_n - e_{n-1}, \ n \geq 2, \quad \omega \in \Omega. 
    $$
    Dann gilt offensichtlich $S_n(\omega) = e_n$ für alle $\omega \in \Omega$ und da $(e_n)_{n \in \N}$ eine Orthonormalbasis von $E$ ist, gilt für alle $z \in E$
    $$
        \lim_{n \to \infty}\langle z,S_n \rangle = 0 = \langle z,S \rangle,
    $$
    wobei $S(\omega) = 0$ für alle $\omega \in \Omega$. Nach dem Satz von Fréchet-Riesz gilt also 
    $$
        \forall f \in E': \quad f(S_n) \stochastisch f(S). 
    $$
    Wegen $\norm{S_n} = \norm{e_n} =1$ für alle $n \in \N$ konvergiert $(S_n)_{n \in \N}$ aber weder fast-sicher noch stochastisch gegen $S$. 
    \qexampled 
\end{remark}

Für Folgen $(a_n)_{n \in \N}$ in $[0, \infty)$ kennt man aus der reellen Analysis die Äquivalenz
$$
    \bigg(\sum_{i=1}^n a_i\bigg)_{n \in \N} \text{ konvergiert. } \iff \bigg(\sum_{i=1}^n a_i\bigg)_{n \in \N} \text{ ist beschränkt.}
$$
Mit Hilfe des Satzes von Itô-Nisio können wir nun ein ähnliches Resultat für symmetrische Zufallsvariablen formulieren. 

\begin{mydef}
    Eine Folge $(X_n)_{n \in \N}$ in $\mathcal{L}_0(E)$ heißt \textit{stochastisch beschränkt}, falls
    $$
        \forall \varepsilon >0 \ \exists R > 0: \quad \sup_{n \in \N}\prob{\norm{X_n} > R} < \varepsilon. 
    $$
\end{mydef}

\begin{lemma}
    Sei $(X_n)_{n \in \N}$ eine Folge von Radon-Zufallsvariablen und $X \in \mathcal{L}_0(E)$ mit $X_n \stochastisch X$. Dann ist $(X_n)_{n \in \N}$ stochastisch beschränkt. 
\end{lemma}
\begin{proof*}
    Sei $\varepsilon > 0$. Dann existiert nach Satz $2.8$ ein $N \in \N$ mit 
    $$
        \sup_{n \geq N}\prob{\norm{X_n - X} > \varepsilon} < \frac{\varepsilon}{2}.
    $$
    Wie man leicht einsieht, existiert ferner ein $R_0 > 0$ mit 
    $$
        \prob{ \norm{X} > R_0} < \frac{\varepsilon}{2} \ \text{ und } \ \max_{1 \leq n \leq N-1}\prob{\norm{X_n} > R_0} < \varepsilon.
    $$
    Für $R := \max\{R_0, \varepsilon\}$ gilt also 
    \begin{align*}
        \sup_{n \geq N}\prob{\norm{X_n} > R} &\leq \sup_{n \geq N}\prob{\norm{X_n - X} + \norm{X} > R} \\\
                                            &\leq \sup_{n \geq N}\prob{\norm{X_n - X} > \frac{R}{2}} + \prob{\norm{X} > \frac{R}{2}} \\\
                                            &< \sup_{n \geq N}\prob{\norm{X_n - X} > \frac{R}{2}} + \frac{\varepsilon}{2} < \varepsilon. 
    \end{align*}
    Zudem gilt nach Konstruktion
    $$
        \max_{1\leq n \leq N-1}\prob{\norm{X_n} > R} < \varepsilon. 
    $$
    Insgesamt erhalten wir somit 
    $$
        \sup_{n \in \N} \prob{\norm{X_n}>R}= \max\bigg(\max_{1\leq n \leq N-1}\prob{\norm{X_n} > R},\sup_{n \geq N}\prob{\norm{X_n}> R}\bigg) < \varepsilon. 
    $$
    \qed 
\end{proof*}
\begin{corollary}
    Sei $d \in \N$ und $(X_n)_{n \in \N}$ eine unabhängige und symmetrische Folge $\R^d$-wertiger Zufallsvariablen. Dann sind äquivalent: 
    \begin{enumerate}[(i)]
        \item $(S_n)_{n \in \N}$ konvergiert fast sicher. 
        \item $(S_n)_{n \in \N}$ ist stochastisch beschränkt. 
    \end{enumerate} 
\end{corollary}
\begin{proof*}
    Zu $(i) \Rightarrow (ii)$ Nach Korollar $2.7$ konvergiert $(S_n)_{n \in \N}$ insbesondere stochastisch und nach Lemma $3.12$ ist $(S_n)_{n \in \N}$ folglich stochastisch beschränkt.   
    \newline Zu $(ii) \Rightarrow (i)$: Aus $(ii)$ erhalten wir unmittelbar
    $$
        \forall \varepsilon > 0 \ \exists R > 0: \quad \inf_{n \in \N}\prob{S_n \in \overline{B}(0,R)} \geq 1 - \varepsilon.
    $$
    Da abgeschlossene und beschränkte Teilmengen des $\R^d$ nach dem Satz von Heine-Borel kompakt sind, folgt daraus die gleichmäßige Straffheit von $(\mu_n)_{n \in \N}$. 
    Nach Satz $3.9$ konvergiert $(S_n)_{n \in \N}$ also fast sicher. \qed 
\end{proof*}



