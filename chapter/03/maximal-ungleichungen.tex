\section{Maximalungleichungen}
Um den Satz von Itô-Nisio zu beweisen, benötigen wir noch eine auf P.Lévy zurückgehende Ungleichung für Summen unabhängiger symmetrischer Radon-Zufallsvariablen. 
Die Beweise in diesem Abschnitt orientieren sich an \cite{li-queffelec} und \cite{bauer}. 
\begin{mydef}
    Eine E-wertige Radon-Zufallsvariable $X$ heißt \textit{symmetrisch}, falls $-X$ die selbe Verteilung besitzt wie $X$, d.h.
    \begin{align*}
        \forall A \in \mathcal{B}(E): P(\{X \in A\}) = P(\{-X \in A\}). 
    \end{align*}
\end{mydef}

\begin{remark}
    Nach dem Eindeutigkeitssatz für charakteristische Funktionale $2.16$ ist $X \in \mathcal{L}_0(E)$ genau dann symmetrisch, wenn 
    $$
        \forall f \in E': \quad \widehat{P^X}(f) = \widehat{P^{-X}}(f) = \widehat{P^X}(-f). 
    $$
\end{remark}

\begin{theorem}[Lévys Maximal-Ungleichung]
    Sei $N \in \N$ und seien $X_1,...,X_N \in \mathcal{L}_0(E)$ unabhängige und symmetrische Radon-Zufallsvariablen. Setze 
    \begin{align*}
        S_n := \sum_{i=1}^n X_i, \quad 1 \leq n \leq N. 
    \end{align*}
    Dann gilt für alle $t > 0$
    \begin{align}
        &P\big(\{ \max_{1 \leq n \leq N} \norm{S_n} > t \}\big) \leq 2 P\big(\{\norm{S_N} > t \}\big), \\\
        &P\big(\{ \max_{1 \leq n \leq N} \norm{X_n} > t \}\big) \leq 2 P\big(\{\norm{S_N} > t \}\big).
    \end{align}
\end{theorem}

\begin{proof*}
    Zu $(3.1)$:
    Setze 
    $$
        T_1(\omega) := \inf\{ k \leq N: \norm{S_k} > t \} \in [0, \infty], \quad \omega \in \Omega. 
    $$
    Dann ist $T_1$ messbar, da für jedes $i =1,...,N$ die Menge $\{ \norm{S_i} \leq t\}$ messbar ist. 
    Wir zeigen zunächst
    \begin{align}
        \{T_1 = n\} \subseteq \{\norm{S_N} > t,\  T_1 = n\} \cup \{\norm{2S_n - S_N}>t,\  T_1=n\}, \quad n \in \{1,...,N\}, \\\
        \prob{\norm{S_N} > t, \ T_1=n} = \prob{\norm{2S_n - S_N} > t,\  T_1=n}, \quad n \in \{1,...,N\}. 
    \end{align}
    Zu $(3.3)$:
    Für $\omega \in \Omega$ mit $T_1(\omega) = n$ und $\norm{S_N(\omega)} \leq t$ liefert die umgekehrte Dreiecksungleichung
    $$
        \norm{2S_n(\omega) - S_N(\omega)} \geq 2\norm{S_n(\omega)} - \norm{S_N(\omega)} > 2t - t = t. 
    $$
    Zu $(3.4)$: Setze $\varepsilon_1 = \varepsilon_2 = ... = \varepsilon_n = 1$ und $\varepsilon_{n+1} = ... = \varepsilon_N = -1$, sowie
    $$
        S'_j := \sum_{i=1}^j\varepsilon_i X_i, \quad j \in \{1,...,N\}. 
    $$
    Dann gilt $S_j = S'_j$ für alle $j \leq n$ und 
    $$
        2S_n - S_N = 2 \sum_{i=1}^n X_i - \sum_{i=1}^N X_i = \sum_{i=1}^nX_i - \sum_{i=n+1}^N X_i = S'_N. 
    $$
    Wegen der Symmetrie und Unabhängigkeit von $X_1,...,X_N$ sind $(S_1,...,S_N)$ und $(S'_1,...,S'_N)$ identisch verteilt. Also ergibt sich 
    \begin{align*}
        \prob{\norm{S_N} > t, T_1=n} &= \prob{\norm{S_1} \leq t,...,\norm{S_{n-1}}\leq t, \norm{S_n} > t, \norm{S_N} > t} \\\
                                   &= \prob{\norm{S'_1}\leq t,...,\norm{S'_{n-1}}\leq t, \norm{S'_n} > t, \norm{S'_N} > t} \\\
                                   &= \prob{\norm{2S_n - S_N} > t, T_1=n}. 
    \end{align*}
    Wir erhalten nun mit $(3.3)$ und $(3.4)$ 
    $$
        \prob{T_1=n} \leq 2 \prob{\norm{S_N} > t, \ T_1=n}.
    $$
   Daraus folgt schließlich
    \begin{align*}
        \prob{\max_{1 \leq n \leq N} \norm{S_n} > t} \leq \sum_{n=1}^{N} \prob{T_1=n} 
                                                     &\leq \sum_{n=1}^N2\prob{\norm{S_N} >t, \ T_1=n} \\\
                                                     &= 2 \prob{\norm{S_N}>t, \ T_1 \leq N} \leq 2 \prob{\norm{S_N} > t}. 
    \end{align*}
    Zu $(3.2)$: 
    Setze 
    $$
        T_2(\omega) := \inf\{k \leq N: \norm{X_k(\omega)} > t\} \in [0, \infty], \quad \omega \in \Omega. 
    $$
    Analog zum Beweis von $(3.3)$ und $(3.4)$ zeigt man 
    \begin{align*}
        \{T_2 = n\} \subseteq \{\norm{S_N} > t, T_2 =n\} \cup \{\norm{2X_n - S_N} > t, T_2=n\}, \quad n \in \{1,...,N\}, \\\
        \prob{\norm{S_N} > t, T_2 = n} = \prob{\norm{2X_n - S_N}, T_2 = n}, \quad n \in \{1,...,N\},
    \end{align*}
    unter der Verwendung der Symmetrie und Unabhängigkeit von $X_1,...,X_N$ und folgert daraus letztendlich 
    \begin{align*}
        \prob{\max_{1\leq n \leq N} \norm{X_n} > t} = \sum_{n=1}^N \prob{T_2 = n} 
                                                    &\leq \sum_{n=1}^N 2\prob{\norm{S_N} > t, T=n} \\\
                                                    &= 2 \prob{\norm{S_N}>t, T_2 \leq N} \leq 2 \prob{\norm{S_N} > t}. 
    \end{align*}
    \qed 
\end{proof*}

\begin{corollary}
    Sei $(X_n)_{n \in \N}$ eine Folge unabhängiger und symmetrischer $E$-wertiger Radon-Zufallsvariablen. Definiere
    $$
        S_n := \sum_{i=1}^n X_i, \quad n \in \N. 
    $$
    Falls $S_n \schwach S$ für $S \in \mathcal{L}_0(E)$, dann gilt für $\lambda$-fast-alle $t \in \R$
    $$
        \prob{\sup_{n \in \N}\norm{S_n} > t} \leq 2 \prob{\norm{S} > t}.
    $$
    Wobei $\lambda$ das Lebesgue-Maß auf $\mathcal{B}(\R)$ bezeichnet. 
\end{corollary}

\begin{proof*}
    Für alle $n \in \N$ gilt nach Ungleichung $(3.1)$
    $$
        \prob{\max_{1\leq k \leq n}\norm{S_k} > t} \leq 2 \prob{\norm{S_n} > t}.
    $$
    Und wegen $\norm{S_n} \schwach \norm{S}$ folgt für alle $t \in \R$, in denen die Verteilungsfunktion $F_{\norm{S}}$ von $\norm{S}$ stetig ist 
    $$
        \prob{\sup_{n \in \N}\norm{S_n} > t} = \lim_{n \to \infty}\prob{\max_{1\leq k \leq n}\norm{S_k} > t} \leq 2 \limsup_{n \to \infty}\prob{\norm{S_n} > t} = 2\prob{\norm{S} >t}. 
    $$
    \qed
\end{proof*}

Mit einer ähnlichen Beweismethode wie im Beweis von Satz $3.3$ erhalten wir für nicht-symmetrische Radon-Zufallsvariablen die folgende, auf Giuseppe Ottaviani und Anatoli Skorohod zurückgehende, Maximal-Ungleichung. 
\begin{theorem}[Maximal-Ungleichung von Ottaviani-Skorohod]
    Sei $N \in \N$ und seien $X_1,...,X_N \in \mathcal{L}_0(E)$ unabhängig. Setze 
    $$
        S_k := \sum_{i=1}^kX_i, \quad k = 1,...,N. 
    $$
    Dann gilt für alle $s,t > 0$
    \begin{align}
        P(\{ \max_{1 \leq k \leq N} \norm{S_k} > s + t \}) \leq \frac{P(\{\norm{S_N} > t \})}{1 - \max\limits_{1 \leq k \leq N}P(\{ \norm{S_N - S_k} > s \})} \ . 
    \end{align}
\end{theorem}

\begin{proof*}
    Setze 
    $$
        T(\omega) := \inf\{k \leq N: \norm{S_k(\omega)} > s + t\} \in [0, \infty], \quad \omega \in \Omega. 
    $$
    Dann ist $T$ messbar und es gilt $\{T = k\} \in \sigma(X_1,...,X_k)$. Weiter gilt 
    $$
        \sum_{k=1}^N \prob{T = k} = \prob{\max_{1\leq k \leq N}\norm{S_k} > s+t}.
    $$
    Für $\omega \in \Omega$ mit $T(\omega) = k$ und $\norm{S_N(\omega) - S_k(\omega)} \leq s$ gilt zudem $\norm{S_N(\omega)} > t$, denn mit der umgekehrten Dreiecksungleichung erhält man in diesem Fall
    $$
        s \geq \norm{S_N(\omega) - S_k(\omega)} \geq \big\lvert\norm{S_N(\omega)}-\norm{S_k(\omega)}\big\rvert > (s+t) - \norm{S_N(\omega)}.
    $$
    Die Unabhängigkeit von $\sigma(X_1,...,X_k)$  und $\sigma(X_{k+1},...,X_N)$ für alle $k \in \{1,...,N\}$ liefert schließlich
    \begin{align*}
        \prob{\norm{S_N} > t} &\geq \sum_{k=1}^N \prob{T=k, \norm{S_N} > t}  \\\
                              &\geq \sum_{k=1}^N \prob{T=k, \norm{S_N - S_k} \leq s} \\\
                              &\geq \min_{1\leq k \leq N}\prob{\norm{S_N-S_k} \leq s} \sum_{k=1}^N \prob{T=k}. 
    \end{align*}
    Umstellen und Beachten von 
    $$
        \min_{1\leq k \leq N}\prob{\norm{S_N-S_k} \leq s} = 1 - \max_{1\leq k \leq N}\prob{\norm{S_N - S_k} > s}
    $$
    liefert nun die Behauptung. \qed
\end{proof*}