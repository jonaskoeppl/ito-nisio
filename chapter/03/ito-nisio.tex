\section{Der Satz von Itô-Nisio}
    Wir wollen nun damit beginnen die bisher erarbeitete Technik zur Untersuchung der Konvergenz zufälliger Reihen in Banachräumen anzuwenden.
    Die vorliegenden Beweise orientieren sich an \cite{ito-nisio}, \cite{ledoux-talagrand}, \cite{li-queffelec} und \cite{van-neerven}.  
    \newline \ \newline 
    Sei $(X_n)_{n \in \N}$ eine Folge unabhängiger Zufallsvariablen in $\mathcal{L}_0(E)$. Für $n \in \N$ setze
    \begin{align*}
    S_n := \sum_{i=1}^n X_i, 
    \quad 
    \mu_n := P^{S_n} .
    \end{align*}
\begin{theorem}[Itô-Nisio]
    Es sind äquivalent
    \begin{enumerate}[(i)]
        \item $(S_n)_{n \in \N}$ konvergiert fast sicher, 
        \item $(S_n)_{n \in \N}$ konvergiert stochastisch, 
        \item $(S_n)_{n \in \N}$ konvergiert in Verteilung. 
    \end{enumerate}
\end{theorem}

\begin{proof*}%TODO: Formatierung% 
    Die Implikationen $(i) \Rightarrow (ii) \Rightarrow (iii)$ wurden bereits in Kapitel 2 gezeigt, es genügt also $(ii) \Rightarrow (i)$ und $(iii) \Rightarrow (ii)$ zu zeigen. 
    \newline 
    zu $(ii) \Rightarrow (i)$: \underline{Fall A}: Für alle $n \in \N$ ist $X_n$ symmetrisch verteilt. 
    \newline 
    Für $n, N \in \N$ mit $n < N$ setze 
    \begin{align*}
        Y_{n,N} &:= \max_{n < k \leq N}\norm{S_k - S_n}, \\\
        Y_n     &:= \lim_{N \to \infty} Y_{n,N} = \sup_{k > n} \norm{S_k - S_n}. 
    \end{align*}
    Seien $\varepsilon, t > 0$. Mit dem Cauchy-Kriterium für stochastische Konvergenz und Lévys Maximal-Ungleichung erhalten wir für $N > n \geq n_0 := n_0(\varepsilon,t) \in \N$
    $$
        P(\{ Y_{n, N} > t\}) \leq 2P(\{ \norm{S_N - S_n} > t\}) \leq \varepsilon . 
    $$
    Es folgt somit 
    $$
        P(\{Y_n > t \}) = \lim_{n \to \infty}P(\{Y_{n,N} > t \}) \leq \varepsilon
    $$
    für $n \geq n_0$. Also gilt $Y_n \stochastisch 0$, nach dem Cauchy-Kriterium für fast sichere Konvergenz folgt daraus die fast sichere Konvergenz von $(S_n)_{n \in \N}$. 
    \newline \underline{Fall B}: Allgemeiner Fall.
    \newline 
    Wir verwenden für diesen Fall die Beweistechnik der Symmetrisierung.  
    Betrachte hierzu den Produktraum $(\Omega \times \Omega, \mathcal{A}\otimes\mathcal{A}, P \times P)$. Für eine Zufallsvariable $X$ auf $(\Omega, \mathcal{A}, P)$ bezeichne
    $$
        \overline{X}(\omega_1, \omega_2) := X(\omega_1) - X(\omega_2), \quad (\omega_1, \omega_2) \in \Omega \times \Omega
    $$   
    die Symmetrisierung von $X$. Wie man mittels der charakteristischen Funktionale von $\overline{X}$ und $-\overline{X}$ leicht einsieht ist $\overline{X}$ tatsächlich symmetrisch. 
    Sei nun $S$ eine Zufallsvariable auf $(\Omega, \mathcal{A}, P)$ mit $S_n \stochastisch S$. 
    Dann folgt direkt $\overline{S_n} \stochastisch \overline{S}$, denn für $\varepsilon > 0$ gilt nach Konstruktion
    $$
        (P\times P)(\{ \norm{\overline{S_n} - \overline{S}} > \varepsilon \} ) \leq 2P(\{ \norm{S_n - S} > \frac{\varepsilon}{2} \}).
    $$
    Nach Fall A gilt also insbesondere $\overline{S_n} \fastsicher  \overline{S}$. Daher existiert eine Menge $\Omega^* \in \mathcal{A}\otimes\mathcal{A}$ mit \mbox{$(P\times P)(\Omega^*) = 1$} und
    $$  
        \forall (\omega_1, \omega_2) \in \Omega^*: \quad \overline{S_n}(\omega_1, \omega_2) \text{ konvergiert. }
    $$
    Mit dem Satz von Fubini erhalten wir
    $$
        0 = \int_{\Omega \times \Omega}1 - 1_{\Omega^*} d(P \times P) = \int_{\Omega}\int_{\Omega}1 - 1_{\Omega^*(\omega_2)}(\omega_1)dP(\omega_1)dP(\omega_2). 
    $$
    wobei $\Omega^*(\omega_2) =\{\omega_1\in\Omega: \ (\omega_1, \omega_2) \in \Omega^* \}$. Somit existiert ein $\omega_2 \in \Omega$ mit $P(\Omega^*(\omega_2)) = 1$ und es gilt
    $$
        \forall \omega_1 \in \Omega^*(\omega_2): \quad S_n(\omega_1) - S_n(\omega_2) \ \text{ konvergiert.}
    $$
    Setze nun $x_n := S_n(\omega_2)$, $n \in \N$. Dann existiert eine Zufallsvariable $L$ auf $(\Omega, \mathcal{A}, P)$ mit $S_n - x_n \fastsicher L$. Nach Voraussetzung erhalten wir also 
    $$
        x_n \stochastisch S - L
    $$
    wobei wir $x_n$ für $n \in \N$ als konstante Zufallsvariable auf $(\Omega, \mathcal{A}, P)$ auffassen. Folglich existiert ein $x \in E$ mit 
    $$
        \lim_{n \to \infty}x_n = x
    $$
    und insgesamt erhalten wir $S_n \fastsicher L + x$. 
    \newline 
    zu $(iii) \Rightarrow (ii)$: Für $1 \leq m < n$ bezeichne
    \begin{align*}
        \mu_{m,n} :&= P^{S_n - S_m}
    \end{align*}
    die Verteilung von $S_n - S_m$. Da $(\mu_n)_{n \in \N}$ nach Voraussetzung schwach gegen ein Wahrscheinlichkeitsmaß $\mu$ konvergiert
    ist die Menge $\{\mu_n: n \in \N \}$ relativ kompakt in $(\mathcal{M}(E), \rho)$ und somit nach dem Satz von Prokhorov gleichmäßig straff.
    Folglich existiert zu $\varepsilon > 0$ eine kompakte Teilmenge $K \subseteq E$ mit 
    $$
        \forall n \in \N: \quad \mu_n(K) \geq 1 - \varepsilon. 
    $$
    Wegen der Stetigkeit von 
    $$
        (x,y) \mapsto x - y, \quad (x,y) \in E \times E
    $$
    ist die Menge $\tilde{K} := \{x - y : x,y \in K \}$ wiederum kompakt, also insbesondere messbar, und es gilt
    \begin{align*}
        \mu_{m,n}(\tilde{K}) \geq P(\{S_n \in K, \ S_m \in K\}) \geq 1 - P(\{S_n \in K^c\}) - P(\{S_m \in K^c\}) \geq 1 - 2\varepsilon.
    \end{align*}
    Somit ist auch $\{\mu_{m,n}: m,n \in \N, m < n \}$ gleichmäßig straff und daher relativ kompakt in $(\mathcal{M}(E), \rho)$. 
    Wir zeigen nun
    \begin{align}
        \forall \varepsilon > 0 \ \exists N \in \N: \ \forall n > m \geq N: \quad \prob{\norm{S_n - S_m} < \varepsilon} = \mu_{m,n}(B(0,\varepsilon)) > 1 - \varepsilon.
    \end{align}
    Mit dem Cauchy-Kriterium folgt daraus die stochastische Konvergenz der Folge $(S_n)_{n \in \N}$. Angenommen $(3.6)$ ist nicht erfüllt, dann gilt
    \begin{align*}
        \exists \varepsilon > 0 \ \forall N \in \N: \ \exists n(N) > m(N) \geq N: \mu_{m(N), n(N)}(B(0, \varepsilon)) \leq 1 - \varepsilon.
    \end{align*}
    Da $\{\mu_{m,n}: m,n \in \N, m < n \}$ relativ kompakt ist existiert insbesondere eine Teilfolge von $(\mu_{m(N),n(N)})_{N \in \N}$ die schwach gegen ein Wahrscheinlichkeitsmaß $\nu$ auf $\mathcal{B}(E)$ konvergiert.
    Ohne Einschränkung können wir annehmen, dass bereits $\mu_{m(N),n(N)} \rightharpoonup \nu$ gilt. Da $B(0, \varepsilon)$ offen ist liefert das Portmanteau-Theorem
    \begin{align}
        \nu(B(0, \varepsilon)) \leq \liminf_{N \to \infty}\mu_{m(N),n(N)}(B(0,\varepsilon)) \leq 1 - \varepsilon. 
    \end{align}
    Andererseits gilt für $z \in E'$ wegen der Unabhängigkeit von $(X_n)_{n \in \N}$
    \begin{align*}
        \widehat{\mu_{n(N)}} = \mathbb{E}(e^{iz(S_{n(N)}})) &= \mathbb{E}(e^{iz(S_{m(N)})}e^{iz(S_{n(N)} - S_{m(N)})}) \\\
                                                   &= \mathbb{E}(e^{iz(S_{m(N)})})\mathbb{E}(e^{iz(S_{n(N)} - S_{m(N)})}) \\\
                                                   &= \widehat{\mu_{m(N}}(z)\widehat{\mu_{m(N),n(N)}}(z). 
    \end{align*}
    Mit Grenzübergang $N \to \infty$ folgt daraus wegen $\mu_N \rightharpoonup \mu$ und $\mu_{m(N),n(N)} \rightharpoonup \nu$
    \begin{align*}
        \forall z \in E': \quad \widehat{\mu}(z) = \widehat{\mu}(z) \widehat{\nu}(z)
    \end{align*}
    Wegen der Stetigkeit von $\widehat{\mu}$ und $\widehat{\mu}(0_{E'}) = 1$ existiert ein $r>0$ mit 
    \begin{align*}
        \widehat{\mu}(z) \neq 0, \quad \text{ für alle } z \in E' \text{ mit } \norm{z}_{op} \leq r. 
    \end{align*}
    Also muss 
    \begin{align*}
        \widehat{\nu}(z) = 1, \quad \text{ für alle } z \in E' \text{ mit } \norm{z}_{op} \leq r. 
    \end{align*}
    gelten. Mit Proposition $2.19$ folgt nun $\nu = \delta_0$. Im Widerspruch zu $(3.7)$. Es gilt folglich $(3.6)$ und $(S_n)_{n \in \N}$ konvergiert demnach stochastisch. 
    \qed
\end{proof*}

\begin{remark}
    Mittels der Maximal-Ungleichung von Ottaviani-Skorohod ist auch ein direkter Beweis der Implikation $(ii) \Rightarrow (i)$ möglich. Betrachte hierzu etwa die Ereignisse 
    $$
        A_N := \bigcap_{m \in \N}\{\sup_{n > m} \norm{S_n - S_m} > \frac{1}{N}\}, \quad N \in \N.                                                                                                                        
    $$
    Dann ist $A := (\cup_{N=1}^{\infty} A_N)^c$ das Ereignis, dass $(S_n)_{n \in \N}$ eine Cauchy-Folge ist und man zeigt 
    $$
        \forall N \in \N: \quad P(A_N) = 0. 
    $$
    Ein Beweis mit dieser Vorgehensweise für den skalaren Fall findet sich etwa \cite{bauer}[Theorem 14.2, S.109]. Der allgemeine Fall funktioniert vollkommen analog. \qexampled
\end{remark}



